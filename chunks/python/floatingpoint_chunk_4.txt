That can make a difference in overall accuracy so that the errors do not accumulate to the point where they affect the final total:

>>>```

    
    
    >>> 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 == 1.0
    False
    >>> sum([0.1] * 10) == 1.0
    True
    
```

The [`math.fsum()`](../library/math.html#math.fsum "math.fsum") goes further and tracks all of the “lost digits” as values are added onto a running total so that the result has only a single rounding. This is slower than [`sum()`](../library/functions.html#sum "sum") but will be more accurate in uncommon cases where large magnitude inputs mostly cancel each other out leaving a final sum near zero:

>>>```

    
    
    >>> arr = [-0.10430216751806065, -266310978.67179024, 143401161448607.16,
    ... -143401161400469.7, 266262841.31058735, -0.003244936839808227]
    >>> float(sum(map(Fraction, arr)))  # Exact summation with single rounding
    8.042173697819788e-13
    >>> math.fsum(arr)          # Single rounding
    8.042173697819788e-13
    >>> sum(arr)             # Multiple roundings in extended precision
    8.042178034628478e-13
    >>> total = 0.0
    >>> for x in arr:
    ... total += x          # Multiple roundings in standard precision
    ...
    >>> total              # Straight addition has no correct digits!
    -0.0051575902860057365
    
```

## 15.1. Representation Error[¶](#representation-error "Link to this heading")

This section explains the “0.1” example in detail, and shows how you can perform an exact analysis of cases like this yourself. Basic familiarity with binary floating-point representation is assumed.

_Representation error_ refers to the fact that some (most, actually) decimal fractions cannot be represented exactly as binary (base 2) fractions. This is the chief reason why Python (or Perl, C, C++, Java, Fortran, and many others) often won’t display the exact decimal number you expect.

Why is that? 1/10 is not exactly representable as a binary fraction. Since at least 2000, almost all machines use IEEE 754 binary floating-point arithmetic, and almost all platforms map Python floats to IEEE 754 binary64 “double precision” values. IEEE 754 binary64 values contain 53 bits of precision, so on input the computer strives to convert 0.1 to the closest fraction it can of the form _J_ /2**_N_ where _J_ is an integer containing exactly 53 bits. Rewriting

```

    
    
    1 / 10 ~= J / (2**N)
    
```

as

```

    
    
    J ~= 2**N / 10
    
```

and recalling that _J_ has exactly 53 bits (is `>= 2**52` but `< 2**53`), the best value for _N_ is 56:

>>>```

    
    
    >>> 2**52 <= 2**56 // 10 < 2**53
    True
    
```

That is, 56 is the only value for _N_ that leaves _J_ with exactly 53 bits.